{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Classification with multichannel model\n",
    "A standard Model for document classification contains an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. The kernel size in the convolutional layer is the number of words used in convolution to produce a single grouping parameter. A multi-channel convolutional neural networds uses mulitple versions of a standard mainly with different kernel size. This approach lets to process the document at at different n-grams at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def load_dataset(filename):\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def create_vocab(docs):\n",
    "    vocab = []\n",
    "    for doc in docs:\n",
    "        vocab.extend(doc)\n",
    "    return vocab\n",
    "    \n",
    "def create_tokenizer(docs):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    return tokenizer\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X_y = np.concatenate((X, y[:, np.newaxis]), axis=1) \n",
    "    np.random.shuffle(X_y)\n",
    "    return X_y\n",
    "\n",
    "def encode_pad_documents(tokenizer,max_length, docs):\n",
    "    encoded_docs = tokenizer.texts_to_sequences(docs)\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    return padded_docs\n",
    "\n",
    "def define_model(vocab_size, length):\n",
    "    # channel 1\n",
    "    inputs1 =  keras.Input(shape=(length,))\n",
    "    embedding1 = keras.layers.Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = keras.layers.Dropout(0.5)(conv1)\n",
    "    pool1 = keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = keras.layers.Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 =  keras.Input(shape=(length,))\n",
    "    embedding2 = keras.layers.Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = keras.layers.Dropout(0.5)(conv2)\n",
    "    pool2 = keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = keras.layers.Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 =  keras.Input(shape=(length,))\n",
    "    embedding3 = keras.layers.Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = keras.layers.Dropout(0.5)(conv3)\n",
    "    pool3 = keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = keras.layers.Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = keras.layers.concatenate([flat1, flat2, flat3])\n",
    "    # FCN\n",
    "    dense1 = keras.layers.Dense(10, activation='relu')(merged)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(dense1)\n",
    "    model = keras.models.Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the train ans test sets that are already cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "107/107 [==============================] - 38s 346ms/step - loss: 0.7052 - accuracy: 0.5371 - val_loss: 0.6783 - val_accuracy: 0.4300\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 39s 364ms/step - loss: 0.5957 - accuracy: 0.6884 - val_loss: 0.3090 - val_accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 41s 386ms/step - loss: 0.0680 - accuracy: 0.9838 - val_loss: 0.6495 - val_accuracy: 0.6700\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 42s 397ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 0.9100\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 43s 404ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2390 - val_accuracy: 0.9200\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 38s 357ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9100\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 37s 349ms/step - loss: 6.3825e-04 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 0.9100\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 37s 350ms/step - loss: 5.8864e-04 - accuracy: 1.0000 - val_loss: 0.2327 - val_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 38s 352ms/step - loss: 2.2667e-04 - accuracy: 1.0000 - val_loss: 0.2327 - val_accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 37s 350ms/step - loss: 3.3186e-04 - accuracy: 1.0000 - val_loss: 0.2352 - val_accuracy: 0.9000\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.3751 - accuracy: 0.8600\n",
      "[0.3751285672187805, 0.8600000143051147]\n"
     ]
    }
   ],
   "source": [
    "# load the train and test sets\n",
    "X_train, y_train = load_dataset('movie_reviews_train.pkl')\n",
    "X_test, y_test = load_dataset('movie_reviews_test.pkl')\n",
    "X = X_train + X_test\n",
    "\n",
    "#Create tokenizer\n",
    "tokenizer = create_tokenizer(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max([len(doc) for doc in X])\n",
    "\n",
    "# encoding to padded document\n",
    "X_train = encode_pad_documents(tokenizer, max_length, X_train)\n",
    "X_test = encode_pad_documents(tokenizer, max_length, X_test)\n",
    "\n",
    "# Shuffle data\n",
    "train = shuffle_data(X_train, y_train)\n",
    "test = shuffle_data(X_test, y_test)\n",
    "\n",
    "# split train set into train and valid set\n",
    "train, valid = train[:1700], train[1700:]\n",
    "X_train, y_train = train[:, :-1], train[:, -1:]\n",
    "X_valid, y_valid = valid[:, :-1], valid[:, -1:]\n",
    "X_test, y_test = test[:, :-1], test[:, -1:]\n",
    "\n",
    "# train model and evaluate\n",
    "model = define_model(vocab_size, max_length)\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                                    min_delta=0,\n",
    "                                                    patience=5,\n",
    "                                                    mode='max',\n",
    "                                                    restore_best_weights=True)\n",
    "model.fit([X_train,X_train,X_train], y_train, \n",
    "            epochs=10, \n",
    "            verbose=1, \n",
    "            validation_data=([X_valid,X_valid,X_valid], y_valid),\n",
    "            batch_size=16,\n",
    "            callbacks = [earlystop_callback]\n",
    "            )\n",
    "model.save('model_movie_review_multichannel.h5')\n",
    "print(model.evaluate([X_test,X_test,X_test], y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
